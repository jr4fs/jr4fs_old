
<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="css/bootstrap.min.css">
		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Lato:100,300,400' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Roboto:400,300,100' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400' rel='stylesheet' type='text/css'>

		<script type="text/javascript" src="js/bootstrap.min.js"></script>

		<title>Fuwen's Homepage</title>
	</head>
	<body>
		<div class="container">
			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h1 class="title-super">Fuwen Tan</h1>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-6 text-left">
					<h4 class="header-thin">Researcher</h4>
					<h4 class="header-thin">Samsung AI Center, Cambridge</h4>
				</div>
				<div class="col-md-6 text-right flex-col">
					<h4 class="header-thin">50/60 Station Road, Cambridge, UK</h4>
					<h4 class="header-thin">fuwen.tan@gmail.com</h4>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-12">
					<hr>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>About me</h2>
				</div>
				<div class="col-md-12 text-thin">
					<p>I am a Researcher in the <a href="https://research.samsung.com/aicenter_cambridge">Samsung AI Center, Cambridge (SAIC-Cambridge)</a>, 
						working with <a href="http://www.braismartinez.org/">Dr. Brais Martinez</a> and <a href="https://ytzimiro.github.io/">Dr. Georgios Tzimiropoulos</a>. 
						<!-- working with <a href="http://www.braismartinez.org/">Dr. Brais Martinez</a>. -->
						<!-- At SAIC-Cambridge, we aim to develop breakthrough technologies in the area of video recognition and produce high-impact research targeting the top Computer Vision and Machine Learning venues. -->
						I received my PhD in Computer Science at the <a href="https://engineering.virginia.edu/computer-science-2021-phd-degree-graduates">University of Virginia</a>, where I worked with <a href="http://vicenteordonez.com/">Dr. Vicente Ord&oacute;&ntilde;ez Rom&aacute;n</a>
						on Vision and Language. Here is my <a href="docs/fwtan-cv.pdf">CV</a>.</p>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Updates</h2>
				</div>
				<div class="col-md-12 text-thin">
					<p>[07/2021] Our <a class="blue_link" href="https://arxiv.org/abs/2103.12236">RRT</a> paper is accepted to <a class="blue_link" href="http://iccv2021.thecvf.com/">ICCV 2021</a>. Code and pretrained models are released in <a class="blue_link" href="https://github.com/uvavision/RerankingTransformer">RerankingTransformer</a>.</p>
					<p>[06/2021] I start working as a Researcher in the <a href="https://research.samsung.com/aicenter_cambridge">Samsung AI Center, Cambridge (SAIC-Cambridge)</a>.</p>
					<p>[05/2021] I was recognized as an <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.</p>
					<p>[04/2021] I successfully defended my PhD Dissertation: <a href="https://search.lib.virginia.edu/sources/uva_library/items/6395w778v"> Learning Local Representations of Images and Text</a>. </p>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Research</h2>
				</div>
			</div> <!-- class="row" -->


			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/rrt.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2103.12236">Instance-level Image Retrieval using Reranking Transformers</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://sites.google.com/view/jiangbo-yuan/">Jiangbo Yuan</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>International Conference on Computer Vision (ICCV), 2021.</h5>
					<p style="font-size:90%;">Instance-level image retrieval is the task of searching in a large database for images that match an object in a query image. To address this task, systems usually rely on a retrieval step that uses global image descriptors, and a subsequent step that performs domain-specific refinements or reranking by leveraging operations
					<a href="https://arxiv.org/abs/2103.12236">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/2103.12236">preprint</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/RerankingTransformer">code</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="docs/2021_retrieval.bib">bibtex</a>]</h5>
				</div>
			</div>

			<!-- <div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/proposal.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://fwtan.github.io/docs/Fuwen_phd_proposal.pdf">PhD Proposal: Learning Local Representations of Images and Text</a></h4>
					<p style="font-size:90%;">Images and text inherently exhibit hierarchical structures, e.g. scenes built from objects, sentences built from words. In many computer vision and natural language processing tasks, learning accurate prediction models requires analyzing the correlation of the local primitives of both the input and output data.
					<a href="https://fwtan.github.io/docs/Fuwen_phd_proposal.pdf">...</a>
					</p>
					<h5>[ <a href="https://fwtan.github.io/docs/Fuwen_phd_proposal.pdf">report</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://docs.google.com/presentation/d/1f6woK5ntHQq0muppvKwF6C_uZ6QcnRwFenqxQwP8xKw/edit?usp=sharing">slides</a> ]</h5>
				</div>
			</div> -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/self-paced.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2001.06001">Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised Learning</a></h4>
					<span><a href="http://www.cs.virginia.edu/~pc9za/">Paola Cascante-Bonilla</a>, <strong>Fuwen Tan</strong>, <a href="https://www.cs.virginia.edu/yanjun/">Yanjun Qi</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>AAAI Conference on Artificial Intelligence (AAAI), 2021.</h5>
					<p style="font-size:90%;">Semi-supervised learning aims to take advantage of a large amount of unlabeled data to improve the accuracy of a model that only has access to a small number of labeled examples. We propose curriculum labeling, an approach that exploits pseudo-labeling for propagating labels to unlabeled samples in an iterative and self-paced fashion. This approach is surprisingly simple and effective and surpasses or is comparable with the best methods proposed in the recent literature across all the standard benchmarks for image classification. 
					<a href="https://arxiv.org/abs/2001.06001">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/2001.06001">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/Curriculum-Labeling">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2021_self-paced.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/drilldown.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1911.03826">Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="http://www.cs.virginia.edu/~pc9za/">Paola Cascante-Bonilla</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Xiaoxiao.Guo">Xiaoxiao Guo</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-wuhu">Hui Wu</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sfeng">Song Feng</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>Conf. on Neural Information Processing Systems (NeurIPS), 2019</h5>
					<p style="font-size:90%;">This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. 
					<a href="https://arxiv.org/abs/1911.03826">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/1911.03826">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/DrillDown">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="http://www.cs.virginia.edu/~ft3ex/docs/drilldown_poster.pdf">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2019_drilldown.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/text2scene.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1809.01110">Text2Scene: Generating Compositional Scenes from Textual Descriptions</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sfeng">Song Feng</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, <em style="color:#a00">(~Oral presentation + Best Paper Finalist)</em></h5>
					<h5>Posts from <a href="https://news.developer.nvidia.com/ai-model-can-generate-images-from-natural-language-descriptions/">NVIDIA Developer News</a>, <a href="https://www.ibm.com/blogs/research/2019/06/text2scene-textual-descriptions/">IBM Research Blog</a> </h5>
					<p style="font-size:90%;">We propose Text2Scene, a model that interprets input natural language descriptions in order to generate various forms of compositional scene representations; from abstract cartoon-like scenes to synthetic images. Unlike recent works, our method does not use generative adversarial networks, but a combination of an encoder-decoder model with a semi-parametric retrieval-based approach. 
					<a href="https://arxiv.org/abs/1809.01110">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/1809.01110">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/Text2Scene">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="http://www.cs.virginia.edu/~ft3ex/docs/text2scene_poster.pdf">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="http://www.cs.virginia.edu/~ft3ex/docs/text2scene_slides.key">slides</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_text2scene.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/who_where.jpg">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1706.01021">Where and Who? Automatic Semantic-Aware Person Composition</a></h4>
					<span><strong>Fuwen Tan</strong>, Crispin Bernier, Benjamin Cohen, <a href="http://vicenteordonez.com">Vicente Ordonez</a>, <a href="http://connellybarnes.com">Connelly Barnes</a></span>
					<h5>Winter Conf. on Applications of Computer Vision (WACV), 2018</h5>
					<p style="font-size:90%;">Image compositing is a popular and successful method used to generate realistic yet fake imagery. Much previous work in compositing has focused on improving the appearance compatibility between a given object segment and a background image. However, most previous work does not investigate the topic of automatically selecting compatible segments and predicting their locations and sizes given a background image.
					<a href="https://arxiv.org/abs/1706.01021">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/1706.01021">paper</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_supp.pdf">supplemental PDF</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="https://github.com/fwtan/who_where">code</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_demo.mp4">video</a> ] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_who_where.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/facecollage.png">
				</div>
				<div class="col-md-9 text-thin">
					<h4><a class="blue_link" href="docs/facecollage_preprint.pdf">FaceCollage: A Rapidly Deployable System for Real-time Head Reconstruction for On-The-Go 3D Telepresence</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, Teng Deng, <a href="http://www.ntu.edu.sg/home/asjfcai/">Jianfei Cai</a>, <a href="http://www.ntu.edu.sg/home/astjcham/">Tat Jen Cham </a></span>
			 		<h5>ACM Multimedia (ACM MM, full paper), 2017</h5>
					<p style="font-size:90%;">This paper presents FaceCollage, a robust and real-time system for head reconstruction that can be used to create easy-to-deploy telepresence systems, using a pair of consumer-grade RGBD cameras that provide a wide range of views of the reconstructed user. A key feature is that the system is very simple to rapidly deploy, with autonomous calibration and requiring minimal intervention from the user.
					<a href="docs/facecollage_preprint.pdf">...</a>
					</p>
					<h5>[ <a href="docs/facecollage_preprint.pdf">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="videos/facecollage.m4v">video</a>] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/frp087-poster.pptx">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2017_facecollage.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3 text-thin">
					<img class="img-responsive img-embeded" src="images/kinfilter.png">
				</div>
				<div class="col-md-9 text-thin">
					<h4>High-Quality Kinect Depth Filtering For Real-time 3D Telepresence</h4>
					<span>Mengyao Zhao, <strong>Fuwen Tan</strong>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, <a href="http://www.cse.ust.hk/~cktang/">Chi-Keung Tang</a>, <a href="http://www.ntu.edu.sg/home/asjfcai/">Jianfei Cai</a>, <a href="http://www.ntu.edu.sg/home/astjcham/">Tat Jen Cham</a> </span>
					<h5>Conf. on Multimedia and Expo (ICME), 2013</h5>
					<p style="font-size:90%;">3D telepresence is a next-generation multimedia application, offering remote users an immersive and natural videoÂ­ conferencing environment with real-time 3D graphics. Kinect sensor, a conswner-grade range camera, facilitates the implementation of some recent 3D telepresence systems. However, conventional data filtering methods are insufficient to handle Kinect depth error because such error is quantized <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">...</a>
					</p>
					<h5>[ <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">IEEE Xplorer</a> ] &nbsp&nbsp&nbsp&nbsp[<a href="docs/zhao2013.bib">bibtex</a>]</h5>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/telereg.png">
				</div>
				<div class="col-md-9 text-thin">
					<h4>Field-guided Registration for Feature-conforming Shape Composition</h4>
					<span><a href="https://vcc.tech/~huihuang">Hui Huang</a>, <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>, <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, Yaobin Ouyang, <strong>Fuwen Tan</strong>, <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a></span>
					<h5>ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2012</h5>
					<p style="font-size:90%;">We present an automatic shape composition method to fuse two shape parts which may not overlap and possibly contain sharp features, a scenario often encountered when modeling man-made objects. At the core of our method is a novel field-guided approach to automatically align two input parts in a feature-conforming manner. <a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/field-guided.pdf">...</a>
					</p>
					<h5>[ <a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/">project</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/field-guided.pdf">paper</a>]&nbsp&nbsp&nbsp&nbsp[<a href="docs/huang2012.bib">bibtex</a>] </h5>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Thesis</h2>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/uva.png", height="500">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">PhD Dissertation: Learning Local Representations of Images and Text</a></h4>
					<p style="font-size:90%;">Images and text inherently exhibit hierarchical structures, e.g. scenes built from objects, sentences built from words. In many computer vision and natural language processing tasks, learning accurate prediction models requires analyzing the correlation of the local primitives of both the input and output data. In this thesis, we develop techniques for learning local representations of images and text and demonstrate their effectiveness on visual recognition, retrieval, and synthesis.
					<a href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">...</a>
					</p>
					<h5>[ <a href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">thesis</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://docs.google.com/presentation/d/1MF0lLGM4w9QkDzCGNAUPdZo3cl-AX8vdr8QkHooIUOo/edit?usp=sharing">slides</a> ]</h5>
				</div>
			</div>

			<!-- <div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Service</h2>
				</div>
			</div> class="row" -->

		</div> <!-- class="container" -->
	</body>
</html>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-70676570-2', 'auto');
	ga('send', 'pageview');
</script>