<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="css/bootstrap.min.css">
		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Lato:100,300,400' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Roboto:400,300,100' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400' rel='stylesheet' type='text/css'>

		<script type="text/javascript" src="js/bootstrap.min.js"></script>

		<title>Jaspreet's Homepage</title>
	</head>
	<body>
		<div class="container">
			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h1 class="title-super">Jaspreet Ranjit</h1>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-6 text-left">
					<h4 class="header-thin">M.S in Computer Science</h4>
					<h4 class="header-thin">University of Virginia, Charlottesville</h4>
				</div>
				<div class="col-md-6 text-right flex-col">
					<h4 class="header-thin">1600 Emmet St.</h4>
					<h4 class="header-thin">jr4fs@virignia.edu</h4>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-12">
					<hr>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>About me</h2>
				</div>
				<div class="col-md-12 text-thin">
					<p>I am a Researcher in the <a href="https://www.vislang.ai/">Vision, Language and Learning Lab at UVA/Rice</a>, 
						working with <a href="https://www.vicenteordonez.com/">Dr. Vicente Ord&oacute;&ntilde;ez Rom&aacute;n</a> and  <a href="https://www.rayb.info/">Dr. Baishakhi Ray</a>. 
						<!-- working with <a href="http://www.braismartinez.org/">Dr. Brais Martinez</a>. -->
						<!-- At SAIC-Cambridge, we aim to develop breakthrough technologies in the area of video recognition and produce high-impact research targeting the top Computer Vision and Machine Learning venues. -->
						I am a MS candidate in Computer Science at the <a href="https://engineering.virginia.edu/">University of Virginia</a> Here is my <a href="docs/fwtan-cv.pdf">CV</a>.</p>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Research</h2>
				</div>
			</div> <!-- class="row" -->


			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/rrt.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a>Analyzing Gender Biases in Image Features of CNNs</a></h4>
					<span><strong>Jaspreet Ranjit</strong>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<p style="font-size:90%;"> Although visual recognition models have undergone many advancements in the past decade, they have been shown to reflect and amplify harmful societal biases often stemming from the large amount of data they have been trained on. I have been working on developing a tool that analyzes the implicit feature representations of large models with respect to bias. This tool allows for comparison of implicit feature representations across different models and synthesizes this data for convenient comparison. 
					</p>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/self-paced.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://dl.acm.org/doi/abs/10.1145/3450267.3450544">Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised Learning</a></h4>
					<span><a href="https://www.linkedin.com/in/aron-harder-a81052115/">Aron Harder</a>, <strong>Jaspreet Ranjit</strong>, <a href="https://www.madhurbehl.com/index.html">Madhur Behl</a></span>
					<h5>ICCPS International Conference on Cyber-Physical Systems, 2021.</h5>
					<p style="font-size:90%;">In this paper we propose Scenario2Vector - a Scenario Description Language (SDL)
						based embedding for traffic situations that allows us to automatically search for similar traffic situations from large AV data-sets. We have also created a
						first of its kind, Traffic Scenario Similarity (TSS) dataset which contains human ranking annotations for the similarity between traffic
						scenarios. 
					<a href="https://dl.acm.org/doi/abs/10.1145/3450267.3450544">...</a>
					</p>
					<h5>[ <a href="https://dl.acm.org/doi/abs/10.1145/3450267.3450544">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/sce2vec.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/drilldown.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a>Object Detection for 3D Printed UAV</a></h4>
					<span><strong>Jaspreet Ranjit</strong>, <a href="https://www.linkedin.com/in/david-sheffler-3306a025/">David Sheffler</a></span>
					<p style="font-size:90%;">Developed a prototype of a 3D printed UAV that completes a mission autonomously and designed object detection algorithms on a raspberry pi interface 
					</p>
				</div>
			</div>

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Internships</h2>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/uva.png", height="500">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">PhD Dissertation: Learning Local Representations of Images and Text</a></h4>
					<p style="font-size:90%;">Images and text inherently exhibit hierarchical structures, e.g. scenes built from objects, sentences built from words. In many computer vision and natural language processing tasks, learning accurate prediction models requires analyzing the correlation of the local primitives of both the input and output data. In this thesis, we develop techniques for learning local representations of images and text and demonstrate their effectiveness on visual recognition, retrieval, and synthesis.
					<a href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">...</a>
					</p>
					<h5>[ <a href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">thesis</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://docs.google.com/presentation/d/1MF0lLGM4w9QkDzCGNAUPdZo3cl-AX8vdr8QkHooIUOo/edit?usp=sharing">slides</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/text2scene.png">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1809.01110">Text2Scene: Generating Compositional Scenes from Textual Descriptions</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sfeng">Song Feng</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, <em style="color:#a00">(~Oral presentation + Best Paper Finalist)</em></h5>
					<h5>Posts from <a href="https://news.developer.nvidia.com/ai-model-can-generate-images-from-natural-language-descriptions/">NVIDIA Developer News</a>, <a href="https://www.ibm.com/blogs/research/2019/06/text2scene-textual-descriptions/">IBM Research Blog</a> </h5>
					<p style="font-size:90%;">We propose Text2Scene, a model that interprets input natural language descriptions in order to generate various forms of compositional scene representations; from abstract cartoon-like scenes to synthetic images. Unlike recent works, our method does not use generative adversarial networks, but a combination of an encoder-decoder model with a semi-parametric retrieval-based approach. 
					<a href="https://arxiv.org/abs/1809.01110">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/1809.01110">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/Text2Scene">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="http://www.cs.virginia.edu/~ft3ex/docs/text2scene_poster.pdf">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="http://www.cs.virginia.edu/~ft3ex/docs/text2scene_slides.key">slides</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_text2scene.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/who_where.jpg">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1706.01021">Where and Who? Automatic Semantic-Aware Person Composition</a></h4>
					<span><strong>Fuwen Tan</strong>, Crispin Bernier, Benjamin Cohen, <a href="http://vicenteordonez.com">Vicente Ordonez</a>, <a href="http://connellybarnes.com">Connelly Barnes</a></span>
					<h5>Winter Conf. on Applications of Computer Vision (WACV), 2018</h5>
					<p style="font-size:90%;">Image compositing is a popular and successful method used to generate realistic yet fake imagery. Much previous work in compositing has focused on improving the appearance compatibility between a given object segment and a background image. However, most previous work does not investigate the topic of automatically selecting compatible segments and predicting their locations and sizes given a background image.
					<a href="https://arxiv.org/abs/1706.01021">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/1706.01021">paper</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_supp.pdf">supplemental PDF</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="https://github.com/fwtan/who_where">code</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_demo.mp4">video</a> ] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_who_where.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/facecollage.png">
				</div>
				<div class="col-md-9 text-thin">
					<h4><a class="blue_link" href="docs/facecollage_preprint.pdf">FaceCollage: A Rapidly Deployable System for Real-time Head Reconstruction for On-The-Go 3D Telepresence</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, Teng Deng, <a href="http://www.ntu.edu.sg/home/asjfcai/">Jianfei Cai</a>, <a href="http://www.ntu.edu.sg/home/astjcham/">Tat Jen Cham </a></span>
			 		<h5>ACM Multimedia (ACM MM, full paper), 2017</h5>
					<p style="font-size:90%;">This paper presents FaceCollage, a robust and real-time system for head reconstruction that can be used to create easy-to-deploy telepresence systems, using a pair of consumer-grade RGBD cameras that provide a wide range of views of the reconstructed user. A key feature is that the system is very simple to rapidly deploy, with autonomous calibration and requiring minimal intervention from the user.
					<a href="docs/facecollage_preprint.pdf">...</a>
					</p>
					<h5>[ <a href="docs/facecollage_preprint.pdf">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="videos/facecollage.m4v">video</a>] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/frp087-poster.pptx">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2017_facecollage.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3 text-thin">
					<img class="img-responsive img-embeded" src="images/kinfilter.png">
				</div>
				<div class="col-md-9 text-thin">
					<h4>High-Quality Kinect Depth Filtering For Real-time 3D Telepresence</h4>
					<span>Mengyao Zhao, <strong>Fuwen Tan</strong>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, <a href="http://www.cse.ust.hk/~cktang/">Chi-Keung Tang</a>, <a href="http://www.ntu.edu.sg/home/asjfcai/">Jianfei Cai</a>, <a href="http://www.ntu.edu.sg/home/astjcham/">Tat Jen Cham</a> </span>
					<h5>Conf. on Multimedia and Expo (ICME), 2013</h5>
					<p style="font-size:90%;">3D telepresence is a next-generation multimedia application, offering remote users an immersive and natural video­ conferencing environment with real-time 3D graphics. Kinect sensor, a conswner-grade range camera, facilitates the implementation of some recent 3D telepresence systems. However, conventional data filtering methods are insufficient to handle Kinect depth error because such error is quantized <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">...</a>
					</p>
					<h5>[ <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">IEEE Xplorer</a> ] &nbsp&nbsp&nbsp&nbsp[<a href="docs/zhao2013.bib">bibtex</a>]</h5>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/telereg.png">
				</div>
				<div class="col-md-9 text-thin">
					<h4>Field-guided Registration for Feature-conforming Shape Composition</h4>
					<span><a href="https://vcc.tech/~huihuang">Hui Huang</a>, <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>, <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, Yaobin Ouyang, <strong>Fuwen Tan</strong>, <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a></span>
					<h5>ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2012</h5>
					<p style="font-size:90%;">We present an automatic shape composition method to fuse two shape parts which may not overlap and possibly contain sharp features, a scenario often encountered when modeling man-made objects. At the core of our method is a novel field-guided approach to automatically align two input parts in a feature-conforming manner. <a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/field-guided.pdf">...</a>
					</p>
					<h5>[ <a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/">project</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/field-guided.pdf">paper</a>]&nbsp&nbsp&nbsp&nbsp[<a href="docs/huang2012.bib">bibtex</a>] </h5>
				</div>
			</div> <!-- class="row" -->

		</div> <!-- class="container" -->
	</body>
</html>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-70676570-2', 'auto');
	ga('send', 'pageview');
</script>